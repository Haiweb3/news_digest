"""
æ–°é—»æŠ“å–æ¨¡å— - ä» RSS æºè·å–å…¨çƒæ–°é—»ï¼ˆæŒ‰ä¸»é¢˜å’Œåœ°åŒºåˆ†ç±»ï¼‰
"""

from __future__ import annotations

import feedparser
import re
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List, Dict, Tuple
from urllib.error import HTTPError, URLError
from urllib.request import Request, urlopen

import config

# ä¸»é¢˜åˆ†ç±»é…ç½®
CATEGORIES = {
    "finance": ("é‡‘èè´¢ç»", "ğŸ’°"),
    "politics": ("å›½é™…æ”¿æ²»", "ğŸŒ"),
    "tech": ("ç§‘æŠ€åŠ¨æ€", "ğŸ”¬"),
    "crypto": ("å¸åœˆå¿«è®¯", "â‚¿"),
    "other": ("å…¶ä»–è¦é—»", "ğŸ“°")
}

# åœ°åŒºåˆ†ç±»é…ç½®
REGIONS = {
    "usa": ("ç¾å›½", "ğŸ‡ºğŸ‡¸"),
    "europe": ("æ¬§æ´²", "ğŸ‡ªğŸ‡º"),
    "japan_korea": ("æ—¥éŸ©", "ğŸ‡¯ğŸ‡µğŸ‡°ğŸ‡·"),
    "aunz": ("æ¾³æ–°", "ğŸ‡¦ğŸ‡ºğŸ‡³ğŸ‡¿"),
    "global": ("å…¨çƒ", "ğŸŒ")
}


def _clean_text(s: str) -> str:
    s = re.sub(r"<[^>]+>", "", s or "")
    s = re.sub(r"\s+", " ", s).strip()
    return s


def _normalize_key(link: str, title: str) -> str:
    link = (link or "").strip()
    if link:
        return f"link:{link}"
    return f"title:{(title or '').strip().lower()}"


def fetch_news_from_rss(url: str, source_name: str, limit: int = 3) -> List[Dict]:
    """ä»å•ä¸ª RSS æºè·å–æ–°é—»ï¼ˆå¸¦è¶…æ—¶ä¸è½»é‡é‡è¯•ï¼‰"""
    last_err: Exception | None = None
    for attempt in range(config.RSS_MAX_RETRIES + 1):
        try:
            req = Request(url, headers={"User-Agent": config.RSS_USER_AGENT})
            with urlopen(req, timeout=config.RSS_TIMEOUT) as resp:
                raw = resp.read()

            feed = feedparser.parse(raw)
            # feedparser å¯èƒ½ä¼šè®¾ç½® bozo=1 è¡¨ç¤ºè§£æå¼‚å¸¸ï¼›ä¸å¼ºåˆ¶å¤±è´¥ï¼Œå°½é‡åƒåˆ° entriesã€‚
            news_list: List[Dict] = []
            for entry in feed.entries[:limit]:
                title = entry.get("title", "æ— æ ‡é¢˜")
                link = entry.get("link", "")
                summary = entry.get("summary", entry.get("description", "")) or ""
                news_item = {
                    "title": _clean_text(title)[:200],
                    "link": (link or "").strip(),
                    "summary": _clean_text(summary)[:500],
                    "source": source_name,
                    "published": entry.get("published", "") or "",
                }
                news_list.append(news_item)
            return news_list

        except (HTTPError, URLError, TimeoutError, Exception) as e:
            last_err = e
            if attempt < config.RSS_MAX_RETRIES:
                time.sleep(config.RSS_BACKOFF_SECONDS * (attempt + 1))
                continue
            print(f"è·å– {source_name} æ–°é—»å¤±è´¥: {e}")
            return []

    # ç†è®ºä¸Šä¸ä¼šåˆ°è¿™é‡Œ
    print(f"è·å– {source_name} æ–°é—»å¤±è´¥: {last_err}")
    return []


def _dedup_in_place(all_news: Dict[str, Dict[str, List[Dict]]]) -> None:
    for category_key, regions in all_news.items():
        for region_key, items in regions.items():
            seen: set[str] = set()
            out: List[Dict] = []
            for it in items:
                k = _normalize_key(it.get("link", ""), it.get("title", ""))
                if k in seen:
                    continue
                seen.add(k)
                out.append(it)
            all_news[category_key][region_key] = out


def fetch_all_news(categories: List[str] | None = None) -> Dict[str, Dict[str, List[Dict]]]:
    """è·å–æ–°é—»æºçš„æ–°é—»ï¼ŒæŒ‰ä¸»é¢˜å’Œåœ°åŒºåˆ†ç±»

    Args:
        categories: éœ€è¦æŠ“å–çš„åˆ†ç±» key åˆ—è¡¨ï¼ˆå¦‚ ["finance", "tech"]ï¼‰ã€‚
            ä¸ºç©ºåˆ™æŠ“å–å…¨éƒ¨åˆ†ç±»ã€‚
    """
    all_news = {}

    selected_categories = categories or list(CATEGORIES.keys())

    tasks: List[Tuple[str, str, str, str]] = []  # (category_key, region_key, source_name, url)
    for category_key in selected_categories:
        if category_key not in CATEGORIES:
            continue
        all_news[category_key] = {}
        category_sources = config.NEWS_SOURCES.get(category_key, {})
        for region_key, sources in category_sources.items():
            all_news[category_key][region_key] = []
            for source in sources:
                tasks.append((category_key, region_key, source["name"], source["url"]))

    with ThreadPoolExecutor(max_workers=config.RSS_MAX_WORKERS) as ex:
        future_map = {
            ex.submit(fetch_news_from_rss, url, source_name, config.NEWS_PER_SOURCE): (category_key, region_key, source_name)
            for (category_key, region_key, source_name, url) in tasks
        }

        for fut in as_completed(future_map):
            category_key, region_key, source_name = future_map[fut]
            try:
                news = fut.result()
            except Exception as e:
                print(f"è·å– {source_name} æ–°é—»å¤±è´¥: {e}")
                news = []
            all_news[category_key][region_key].extend(news)
            print(f"ä» {source_name} è·å–äº† {len(news)} æ¡æ–°é—»")

    _dedup_in_place(all_news)
    return all_news


def format_news_for_summary(news_data: Dict[str, Dict[str, List[Dict]]]) -> str:
    """å°†æ–°é—»æ ¼å¼åŒ–ä¸ºæ–‡æœ¬ï¼Œä¾› AI æ€»ç»“"""
    parts: List[str] = []

    for category_key, (category_name, cat_emoji) in CATEGORIES.items():
        category_news = news_data.get(category_key, {})
        if not category_news:
            continue

        parts.append(f"=== {category_name} ===\n")

        # å¸åœˆä¸åˆ†åœ°åŒº
        if category_key == "crypto":
            global_news = category_news.get("global", [])
            for i, news in enumerate(global_news, 1):
                parts.append(f"{i}. [{news['source']}] {news['title']}")
                parts.append(f"   {news['summary']}\n")
        else:
            # å…¶ä»–åˆ†ç±»æŒ‰åœ°åŒºæ˜¾ç¤º
            for region_key in ["usa", "europe", "japan_korea", "aunz"]:
                region_news = category_news.get(region_key, [])
                if region_news:
                    region_name, region_emoji = REGIONS[region_key]
                    parts.append(f"--- {region_emoji} {region_name} ---")
                    for i, news in enumerate(region_news, 1):
                        parts.append(f"{i}. [{news['source']}] {news['title']}")
                        parts.append(f"   {news['summary']}\n")

        parts.append("")

    return "\n".join(parts)


def count_total_news(news_data: Dict[str, Dict[str, List[Dict]]]) -> int:
    """ç»Ÿè®¡æ€»æ–°é—»æ•°"""
    total = 0
    for category_news in news_data.values():
        for region_news in category_news.values():
            total += len(region_news)
    return total


def print_news_stats(news_data: Dict[str, Dict[str, List[Dict]]]):
    """æ‰“å°æ–°é—»ç»Ÿè®¡"""
    for cat_key, (cat_name, cat_emoji) in CATEGORIES.items():
        category_news = news_data.get(cat_key, {})
        cat_total = sum(len(r) for r in category_news.values())
        print(f"   {cat_emoji} {cat_name}: {cat_total} æ¡")

        if cat_key != "crypto":
            for region_key in ["usa", "europe", "japan_korea", "aunz"]:
                region_news = category_news.get(region_key, [])
                if region_news:
                    region_name, region_emoji = REGIONS[region_key]
                    print(f"      {region_emoji} {region_name}: {len(region_news)} æ¡")


if __name__ == "__main__":
    print("æ­£åœ¨è·å–æ–°é—»...")
    news = fetch_all_news()
    print("\n" + "="*50)
    print(f"å…±è·å– {count_total_news(news)} æ¡æ–°é—»")
    print_news_stats(news)
    print("="*50)
